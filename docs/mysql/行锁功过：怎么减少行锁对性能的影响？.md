## 行锁功过：怎么减少行锁对性能的影响？

## 
::: tip
 锁粒度越大，开销越小，锁冲突的概率越小，安全性也就越高，但业务并发度以及性能越差；

 反之锁粒度越小，开销也就越大，锁冲突的概率越大（易导致死锁），安全性也就越低，但业务并发度以及性能越好。

 MyISAM 引擎就`不支持行锁`。不支持行锁`意味着`并发控制`只能使用表锁`
::: 


### 两阶段锁协议。
::: tip
 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是`要等到事务结束时才释放`。这个就是`两阶段锁协议`。
::: 
如下面这个图所示，事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行

![img](/mysql/base/51f501f718e420244b0a2ec2ce858710.jpg)
::: warning
 在提交事务的时候，`位置越在前`的语句被行锁`锁住的时间就越长`，`位置越在后`的语句被行锁`锁住的时间就越短`，找到`一个事务中并发修改的语句`将其放置在`最后`将提高并发度

 尽量将并发度高的行,放在事务的最后进行执行
::: 
### 死锁和死锁检测

来个栗子

![img](/mysql/base/4d0eeec7b136371b79248a0aed005a52.jpg)
::: tip
 这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。

 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。
::: 
解决：

- 一种策略是`超时等待`，直接进入等待，直到超时。这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置，默认值是 50s。
- 另一种策略是，`发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务`，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 `on`,`默认开启`，表示开启这个逻辑。
::: tip
 对于第一种时间上不好把握，50s显然太长无法接收，如果设置成1s很多正常执行的因为延迟也被错杀，正常情况下我们采用第二种策略，但是它也是有额外负担的。
::: 
每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此`循环`，最后判断是否出现了`循环`等待，也就是死锁。

假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的(1000X1000)。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。

### 怎么解决由这种热点行更新导致的性能问题呢？
::: tip
高并发下避免死锁检测带来的负面影响： 

1. 确保业务上不会产生死锁，直接将死锁检测关闭。（innodb 自带死锁检测） 
2. 在`数据库中间件`中统一对更新同一行的请求`进行排队`，控制并发度。 
3. 业务逻辑上进行优化，将一行数据分解成多行，降低写入压力。(推荐)

对于第三条解释一下：比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。
::: 




提问：

如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：

- 第一种，直接执行 delete from T limit 10000;
- 第二种，在一个连接中循环执行 20 次 delete from T limit 500;
- 第三种，在 20 个连接中同时执行 delete from T limit 500。



解释：

第二种比较好

第一种方式（即：直接执行 delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。

第三种方式（即：在 20 个连接中同时执行 delete from T limit 500），会人为造成锁冲突。

